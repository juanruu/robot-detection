{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h-7XMM8L6vIu"
   },
   "source": [
    "# Training a Computer Vision model with PyTorch\n",
    "PyTorch is an open-source machine learning framework developed primarily by Facebook's AI Research lab (FAIR). It is designed to provide a flexible and dynamic platform for building and training machine learning models, particularly deep neural networks. We are going to use PyTorch to implement a neural network to detect our robots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2uocshQQ7j28",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "rLGVo8uq7l-2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dfc/projects/classes/pids/pids/lab/venv/lib/python3.8/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/home/dfc/projects/classes/pids/pids/lab/venv/lib/python3.8/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import xml.etree.ElementTree as ET\n",
    "from typing import Tuple, Union, Any\n",
    "from torchvision.models import detection\n",
    "import torchvision.transforms.v2 as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Udf9DXRD7o2H",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Utils functions\n",
    "We are going to define some functions that will be used during the network training. Read the docstrings (comments explaining the functions) to understand them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8RH3c7Th7pEV"
   },
   "outputs": [],
   "source": [
    "def set_seed(seed: int) -> None:\n",
    "    \"\"\"\n",
    "    Define a seed for reproducibility. It allows experiment repetition obtaining the exact same results.\n",
    "    :param seed: integer number indicating which seed you want to use.\n",
    "    :return: None.\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
    "    np.random.seed(seed)  # Numpy module.\n",
    "    # random.seed(seed)  # Python random module.\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "def get_inner_model(model: detection) -> Any:\n",
    "    \"\"\"\n",
    "    PyTorch provides a model wrapper to enable multiple GPUs. This function returns the inner model (without wrapper).\n",
    "    :param model: Torch model, with or without nn.DataParallel wrapper.\n",
    "    :return: if model is wrapped, it returns the inner model (model.module). Otherwise, it returns the input model.\n",
    "    \"\"\"\n",
    "    return model.module if isinstance(model, torch.nn.DataParallel) else model\n",
    "\n",
    "\n",
    "def torch_load_cpu(load_path: str) -> Any:\n",
    "    \"\"\"\n",
    "    Load the data saved from a trained model (model weights, optimizer state, last epoch number to resume training...)\n",
    "    :param load_path: string indicating the path to the data saved from a trained model.\n",
    "    :return: dictionary containing data saved from a trained model.\n",
    "    \"\"\"\n",
    "    return torch.load(load_path, map_location=lambda storage, loc: storage)  # Load on CPU\n",
    "\n",
    "\n",
    "def load_model_path(path: str, model: detection, device: torch.device, optimizer: torch.optim = None) -> Tuple[Any, Any, int]:\n",
    "    \"\"\"\n",
    "    Load the trained weights of a model into the given model.\n",
    "    :param path: string indicating the path to the trained weights of a model.\n",
    "    :param model: the model where you want to load the weights.\n",
    "    :param device: whether gpu or cpu is being used.\n",
    "    :param optimizer: the optimizer initialized before loading the weights.\n",
    "    :return:\n",
    "        model: Torchvision model.\n",
    "        optimizer: Torch optimizer.\n",
    "        history: training history.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load model state\n",
    "    load_data = torch_load_cpu(path)\n",
    "    model.load_state_dict({**model.state_dict(), **load_data.get('model', {})})\n",
    "    # model_ = get_inner_model(model)\n",
    "    # model_.load_state_dict({**model_.state_dict(), **load_data.get('model', {})})\n",
    "\n",
    "    # Load rng state\n",
    "    torch.set_rng_state(load_data['rng_state'])\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.set_rng_state_all(load_data['cuda_rng_state'])\n",
    "\n",
    "    # Load optimizer state\n",
    "    if 'optimizer' in load_data and optimizer is not None:\n",
    "        optimizer.load_state_dict(load_data['optimizer'])\n",
    "        for state in optimizer.state.values():\n",
    "            for k, v in state.items():\n",
    "                if torch.is_tensor(v):\n",
    "                    state[k] = v.to(device)\n",
    "\n",
    "    # Get initial epoch and training history\n",
    "    history = load_data['history']\n",
    "\n",
    "    return model, optimizer, history\n",
    "\n",
    "\n",
    "def torchvision_model(model_name: str, pretrained: bool = False, num_classes: int = 2) -> Any:\n",
    "    \"\"\"\n",
    "    Return a model from a list of Torchvision models.\n",
    "    :param model_name: name of the Torchvision model that you want to load.\n",
    "    :param pretrained: whether pretrained weights are going to be loaded or not.\n",
    "    :param num_classes: number of classes. Minimum is 2: 0 = background, 1 = object.\n",
    "    :return:\n",
    "        model: Torchvision model.\n",
    "    \"\"\"\n",
    "\n",
    "    # Torchvision models\n",
    "    model_dict = {\n",
    "        'faster_rcnn_v1': detection.fasterrcnn_resnet50_fpn,\n",
    "        'faster_rcnn_v2': detection.fasterrcnn_resnet50_fpn_v2,\n",
    "        'faster_rcnn_v3': detection.fasterrcnn_mobilenet_v3_large_fpn,\n",
    "        # 'faster_rcnn_v4': detection.fasterrcnn_mobilenet_v3_large_320_fpn,\n",
    "        # 'fcos_v1': detection.fcos_resnet50_fpn,\n",
    "        'retinanet_v1': detection.retinanet_resnet50_fpn,\n",
    "        'retinanet_v2': detection.retinanet_resnet50_fpn_v2,\n",
    "        'ssd_v1': detection.ssd300_vgg16,\n",
    "        'ssd_v2': detection.ssdlite320_mobilenet_v3_large,\n",
    "    }\n",
    "\n",
    "    # Create model and load pretrained weights (if pretrained=True)\n",
    "    if model_name in model_dict:\n",
    "        model = model_dict[model_name](weights='COCO_V1' if pretrained else None)\n",
    "\n",
    "        # Modify the model's output layer for the number of classes in your dataset\n",
    "        if 'faster_rcnn' in model_name:\n",
    "            in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "            model.roi_heads.box_predictor = detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)\n",
    "        elif 'retinanet' in model_name:\n",
    "            in_features = model.head.classification_head.cls_logits.in_channels\n",
    "            num_anchors = model.head.classification_head.num_anchors\n",
    "            model.head.classification_head = detection.retinanet.RetinaNetClassificationHead(\n",
    "                in_features, num_anchors, num_classes\n",
    "            )\n",
    "        elif 'fcos' in model_name:\n",
    "            in_features = model.head.classification_head.cls_logits.in_channels\n",
    "            num_anchors = model.head.classification_head.num_anchors\n",
    "            model.head.classification_head = detection.fcos.FCOSClassificationHead(\n",
    "                in_features, num_anchors, num_classes\n",
    "            )\n",
    "        elif 'ssd_v1' in model_name:\n",
    "            in_features = [module.in_channels for module in model.head.classification_head.module_list]\n",
    "            num_anchors = model.anchor_generator.num_anchors_per_location()\n",
    "            model.head.classification_head = detection.ssd.SSDClassificationHead(\n",
    "                in_features, num_anchors, num_classes\n",
    "            )\n",
    "        elif 'ssd_v2' in model_name:\n",
    "            in_features = [module[0][0].in_channels for module in model.head.classification_head.module_list]\n",
    "            num_anchors = model.anchor_generator.num_anchors_per_location()\n",
    "            model.head.classification_head = detection.ssd.SSDClassificationHead(\n",
    "                in_features, num_anchors, num_classes\n",
    "            )\n",
    "\n",
    "    # Error: Model not in list\n",
    "        else:\n",
    "            assert False, 'Model {} not in list. Indicate a Torchvision model from the list.'.format(model_name)\n",
    "    else:\n",
    "        assert False, 'Model {} not in list. Indicate a Torchvision model from the list.'.format(model_name)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_model(\n",
    "    model_name: str,\n",
    "    model_path: str = '',\n",
    "    num_classes: int = 2,\n",
    "    lr_data: list = None,\n",
    "    pretrained: bool = False,\n",
    "    use_gpu: bool = False\n",
    ") -> Tuple[Any, Any, int, torch.device]:\n",
    "    \"\"\"\n",
    "    Main function to create and load the model.\n",
    "    :param model_name: name of the Torchvision model to load.\n",
    "    :param model_path: path to the model.\n",
    "    :param num_classes: number of classes. Minimum is 2: 0 = background, 1 = object.\n",
    "    :param lr_data: list containing [learning rate, learning rate momentum, learning rate decay].\n",
    "    :param pretrained: whether Torch pretrained weights on COCO dataset are going to be used or not.\n",
    "    :param use_gpu: whether to use GPU or CPU.\n",
    "    :return:\n",
    "        model: Torch model.\n",
    "        optimizer: Torch optimizer.\n",
    "        history: training history.\n",
    "        device: torch device indicating whether to use GPU or CPU.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define device (GPU or CPU)\n",
    "    device_name = 'cuda' if use_gpu and torch.cuda.is_available() else 'cpu'\n",
    "    device = torch.device(device_name)\n",
    "\n",
    "    # Load Torchvision model\n",
    "    model = torchvision_model(model_name, pretrained, num_classes).to(device)\n",
    "    # if use_gpu and torch.cuda.is_available() and torch.cuda.device_count() > 1:\n",
    "    #     model = torch.nn.DataParallel(model).cuda()\n",
    "\n",
    "    # Define the optimizer\n",
    "    if lr_data:\n",
    "        params = [p for p in model.parameters() if p.requires_grad]\n",
    "        optimizer = torch.optim.SGD(params, lr=lr_data[0], momentum=lr_data[1], weight_decay=lr_data[2])\n",
    "    else:\n",
    "        optimizer = None\n",
    "\n",
    "    # Load trained weights, optimizer state, and initial epoch\n",
    "    if os.path.isfile(model_path):\n",
    "        print(f\"WARNING!!! -> You are going to overwrite the model in {model_path}.\")\n",
    "        print(f\"You should only do this in case you want to resume a training. Are you sure you want to overwrite it?\")\n",
    "        print(f\"If not, change the model_path variable to a non-existing path.\\n\")\n",
    "        print(f\"\\t[*] Loading Torch model from '{model_path}'\")\n",
    "        model, optimizer, history = load_model_path(model_path, model, device, optimizer)\n",
    "    else:\n",
    "        history = {'train_loss': [], 'valid_loss': [], 'best_value': np.inf, 'current_epoch': 0}\n",
    "        print(f\"Model weights not found in '{model_path}'. Training from scratch.\")\n",
    "\n",
    "    return model, optimizer, history, device\n",
    "\n",
    "def clip_grad_norms(param_groups, max_norm=np.inf) -> Tuple[list, list]:\n",
    "    \"\"\"\n",
    "    Limit (clip) the norm of the gradients to avoid gradient explosion.\n",
    "    :param param_groups: parameters of the optimizer, from which gradients are extracted.\n",
    "    :param max_norm: maximum value for the norm of the gradient. max_norm = 0 avoids clipping.\n",
    "    :return:\n",
    "        grad_norms: gradients.\n",
    "        grad_norms_clipped: clipped gradients.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get gradients\n",
    "    grad_norms = [\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            group['params'],\n",
    "            max_norm if max_norm > 0 else np.inf,  # Inf so no clipping but still call to calc\n",
    "            norm_type=2\n",
    "        )\n",
    "        for group in param_groups\n",
    "    ]\n",
    "\n",
    "    # Clip gradients\n",
    "    grad_norms_clipped = [min(g_norm, max_norm) for g_norm in grad_norms] if max_norm > 0 else grad_norms\n",
    "    return grad_norms, grad_norms_clipped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PGRBg6KI_Abw"
   },
   "source": [
    "## Data Generator\n",
    "Data generators (called Datasets in Torch) are used to iteratively load images and annotations to feed neural networks during train, validation, and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WAgWT9fR_SOH"
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, path_dataset: str, resize_shape: tuple = None, transform: transforms.Compose = None) -> None:\n",
    "        \"\"\"\n",
    "        Custom dataset that feeds the network during train, validation, and test.\n",
    "        :param path_dataset: path to the dataset.\n",
    "        :param resize_shape: tuple indicating height and width to resize images (for faster performance).\n",
    "        :param transform: list of transforms to apply to the images.\n",
    "        \"\"\"\n",
    "        print(\"Loading annotations...\")\n",
    "        self.annotations = parse_annotations(path_dataset)\n",
    "        self.resize_shape = resize_shape\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, dict]:\n",
    "        \"\"\"\n",
    "        Get an index corresponding to one of the images and return the image and its annotation.\n",
    "        :param idx: index of image to load.\n",
    "        :return:\n",
    "            image: Torch tensor containing the image with shape (Channels, Height, Width).\n",
    "            targets: dictionary with the bounding boxes (boxes) and class labels (labels) of each annotated object.\n",
    "        \"\"\"\n",
    "        # Get one annotation for the current index\n",
    "        annotation = self.annotations[idx]\n",
    "\n",
    "        # Load image\n",
    "        image = Image.open(annotation['path_image']).convert(\"RGB\")\n",
    "\n",
    "        # Load bounding boxes\n",
    "        boxes = self.annotations[idx]['boxes']\n",
    "        boxes = torch.tensor(boxes)\n",
    "\n",
    "        # Load labels (class of the object)\n",
    "        labels = self.annotations[idx]['labels']\n",
    "        labels = torch.tensor(labels).type(torch.int64)\n",
    "\n",
    "        # If empty, reshape to [0, 4]\n",
    "        if boxes.numel() == 0:\n",
    "            boxes = torch.zeros((0, 4), dtype=torch.float32)\n",
    "\n",
    "        # Apply transforms\n",
    "        if self.transform:\n",
    "            w, h = image.size\n",
    "            image, boxes, class_labels = self.transform(image, boxes, labels)\n",
    "            if self.resize_shape:\n",
    "                boxes = resize_boxes(boxes, self.resize_shape, (h, w))\n",
    "\n",
    "        # Torchvision models use this structure for boxes and labels\n",
    "        targets = {'boxes': boxes, 'labels': torch.tensor(labels)}\n",
    "        return image, targets\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"\n",
    "        Length of the dataset.\n",
    "        :return: number of annotated images contained in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.annotations)\n",
    "\n",
    "\n",
    "def resize_boxes(boxes: torch.Tensor, resize_shape: tuple, image_shape: tuple) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Resize the shape of the bounding boxes when the size of the images is also resized.\n",
    "    :param boxes: Torch tensor containing bounding boxes with format: [x_min, y_min, x_max, y_max].\n",
    "    :param resize_shape: new image size.\n",
    "    :param image_shape: previous image size.\n",
    "    :return:\n",
    "        boxes: resized bounding boxes.\n",
    "    \"\"\"\n",
    "    boxes[:, 0] *= resize_shape[1] / image_shape[1]\n",
    "    boxes[:, 1] *= resize_shape[0] / image_shape[0]\n",
    "    boxes[:, 2] *= resize_shape[1] / image_shape[1]\n",
    "    boxes[:, 3] *= resize_shape[0] / image_shape[0]\n",
    "    return boxes\n",
    "\n",
    "\n",
    "def get_transform(norm: tuple, resize_shape: Union[tuple, None]) -> transforms.Compose:\n",
    "    \"\"\"\n",
    "    Define data transformations and apply them to the dataset.\n",
    "    :param norm: mean and std required by each Torchvision model to normalize the input images.\n",
    "    :param resize_shape: new image size.\n",
    "    :return:\n",
    "        transform: list of transforms to apply to the images.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert images to Torch tensors and apply the previous normalization\n",
    "    t = [transforms.ToTensor(), transforms.Normalize(*norm)]\n",
    "\n",
    "    # Resize images if required\n",
    "    if resize_shape:\n",
    "        t.append(transforms.Resize(resize_shape))\n",
    "    return transforms.Compose(t)\n",
    "\n",
    "\n",
    "def parse_annotations(path_dataset: str) -> list:\n",
    "    \"\"\"\n",
    "    Read dataset structure and extract path to images and annotations.\n",
    "    :param path_dataset: path to the dataset.\n",
    "    :return:\n",
    "        annotations: list of dictionaries, each with the path to the image, the bounding boxes, and the class labels.\n",
    "    \"\"\"\n",
    "\n",
    "    # Search labels on each sequence\n",
    "    annotations = []\n",
    "    for sequence in tqdm(sorted(os.listdir(path_dataset))):\n",
    "        if os.path.isdir(os.path.join(path_dataset, sequence)):\n",
    "            path_sequence = os.path.join(path_dataset, sequence, 'labels')\n",
    "\n",
    "            # Search labels on each frame\n",
    "            for frame in sorted(os.listdir(path_sequence)):\n",
    "                if os.path.isfile(os.path.join(path_sequence, frame)):                \n",
    "                    path_frame_labels = os.path.join(path_sequence, frame)\n",
    "    \n",
    "                    # Load labels\n",
    "                    image_name, boxes = read_label(path_frame_labels)\n",
    "                    # if len(boxes) == 0:\n",
    "                    #     continue\n",
    "    \n",
    "                    # Get path to the image\n",
    "                    path_image = os.path.join(path_dataset, sequence, 'images', image_name)\n",
    "    \n",
    "                    # Save the path to the image, the boxes, and the labels (class of object) in a dictionary\n",
    "                    annotations.append({\n",
    "                        'path_image': path_image,\n",
    "                        'boxes': np.array(boxes),\n",
    "                        'labels': np.array([1 for _ in range(len(boxes))])\n",
    "                    })\n",
    "    return annotations\n",
    "\n",
    "\n",
    "def read_label(xml_file: str) -> Tuple[str, list]:\n",
    "    \"\"\"\n",
    "    Read annotation xml file.\n",
    "    :param xml_file: path to xml file.\n",
    "    :return:\n",
    "        image_name: string with the imnage filename.\n",
    "        list_with_all_boxes: list of bounding boxes.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load xml data\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    # Load frame name\n",
    "    image_name = root.find('filename').text\n",
    "\n",
    "    # Load all bounding boxes\n",
    "    list_with_all_boxes = []\n",
    "    for boxes in root.iter('object'):\n",
    "\n",
    "        # Get box\n",
    "        ymin, xmin, ymax, xmax = None, None, None, None\n",
    "        ymin = int(boxes.find(\"bndbox/ymin\").text)\n",
    "        xmin = int(boxes.find(\"bndbox/xmin\").text)\n",
    "        ymax = int(boxes.find(\"bndbox/ymax\").text)\n",
    "        xmax = int(boxes.find(\"bndbox/xmax\").text)\n",
    "        \n",
    "        # Skip boxes with missing values\n",
    "        if (ymin is None) or (xmin is None) or (ymax is None) or (xmax is None):\n",
    "            continue\n",
    "\n",
    "        # Save box\n",
    "        list_with_one_box = [xmin, ymin, xmax, ymax]\n",
    "        list_with_all_boxes.append(list_with_one_box)\n",
    "\n",
    "    return image_name, list_with_all_boxes\n",
    "\n",
    "\n",
    "def collate_fn(batch: list) -> tuple:\n",
    "    \"\"\"\n",
    "    Avoids stacking images and annotations from dataloader as a Torch tensor, and stacks them as tuples.\n",
    "    :param batch: images and annotations loaded from dataset.\n",
    "    :return:\n",
    "        batch: images and annotations stacked as tuples.\n",
    "    \"\"\"\n",
    "    return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WZmRG2eb_bCL"
   },
   "source": [
    "## Parameters\n",
    "Here, we define multiple parameters to train our network. Read them carefully, understand their utility, and play with them to obtain the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fcSsta2J_WmX",
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "local_path = '../..'\n",
    "\n",
    "# Seed (for reproducibility) -> Do NOT change\n",
    "seed = 0\n",
    "set_seed(seed)\n",
    "\n",
    "# Data parameters\n",
    "path_dataset = f'{local_path}/dataset'                    # Path to dataset\n",
    "train_valid_test_split = [0.8, 0.2, 0.0]                  # Train-Valid-Test split of the dataset\n",
    "\n",
    "# Model parameters\n",
    "model_name = 'faster_rcnn_v1'                             # Torchvision model\n",
    "model_path = f'{local_path}/models/{model_name}/model.pt' # Path to save trained model\n",
    "num_classes = 2                                           # Number of classes (ALWAYS 2: 0=background, 1=robot)\n",
    "resize_shape = None                                       # Resize images for faster performance. None to avoid resizing\n",
    "pretrained = True                                         # Use weights pre-trained on COCO dataset\n",
    "\n",
    "# Train parameters (do NOT change)\n",
    "use_gpu = True                                            # Use GPU (True) or CPU (False)\n",
    "num_workers = 0                                           # Number of workers (CPU cores loading data)\n",
    "\n",
    "# Other train parameters (explain them and play with them)\n",
    "num_epochs = ...                                           # Number of epochs\n",
    "batch_size = ...                                            # Batch size\n",
    "lr = ...                                                # Learning rate\n",
    "lr_momentum = ...\n",
    "lr_decay = ...\n",
    "lr_factor = ...\n",
    "lr_patience = ...\n",
    "lr_threshold = ...\n",
    "lr_min = ...\n",
    "models_to_clip = ['<model-1>', '<model-2>']\n",
    "max_grad_norm = 1 if model_name in models_to_clip else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hd8Z1k-cAzrR"
   },
   "source": [
    "## Train\n",
    "Now, we have enough material to train our network. First, we will create a Torchvision model (and resume previous training if you iterrupted it). We also define a learning rate scheduler to modify the learning rate value while training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3346,
     "status": "ok",
     "timestamp": 1697809267955,
     "user": {
      "displayName": "Dani Fuertes",
      "userId": "03202983811684298813"
     },
     "user_tz": -120
    },
    "id": "k1j7BR6jAcrn",
    "outputId": "14e2ed39-592b-4d07-d305-2659e51a44e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weights not found in '../../models/faster_rcnn_v1/model.pt'. Training from scratch.\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "model, optimizer, history, device = get_model(\n",
    "    model_name=model_name,\n",
    "    model_path=model_path,\n",
    "    num_classes=num_classes,\n",
    "    lr_data=[lr, lr_momentum, lr_decay],\n",
    "    pretrained=pretrained,\n",
    "    use_gpu=use_gpu\n",
    ")\n",
    "\n",
    "# Define learning rate scheduler\n",
    "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer=optimizer,\n",
    "    factor=lr_factor,\n",
    "    patience=lr_patience,\n",
    "    threshold=lr_threshold,\n",
    "    min_lr=lr_min,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YKmqgBXRBrAc"
   },
   "source": [
    "Secondly, let's create a directory to save our future trained model, and a data generator to iteratively load data from your labeled dataset to feed your network. Notice (and understand) the diference in PyTorch between Dataset and DataLoader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "9j-RlsKOAUyN"
   },
   "outputs": [],
   "source": [
    "# Create output directory if not exists\n",
    "save_dir = '/'.join(model_path.split('/')[:-1])\n",
    "os.makedirs(save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "7rVt62DmCEMK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading annotations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 544.96it/s]\n"
     ]
    }
   ],
   "source": [
    "# Image transformations\n",
    "image_and_std = (model.transform.image_mean, model.transform.image_std)\n",
    "transform = get_transform(image_and_std, resize_shape)\n",
    "\n",
    "# Initialize your custom dataset\n",
    "dataset = CustomDataset(\n",
    "    path_dataset=path_dataset,\n",
    "    resize_shape=resize_shape,\n",
    "    transform=transform,\n",
    ")\n",
    "\n",
    "# Apply Train-Valid-Test split\n",
    "train_set, valid_set, test_set = torch.utils.data.random_split(\n",
    "    dataset=dataset,\n",
    "    lengths=train_valid_test_split,\n",
    "    generator=torch.Generator().manual_seed(seed)\n",
    ")\n",
    "\n",
    "# Initialize your dataloaders\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "valid_loader = DataLoader(\n",
    "    dataset=valid_set,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    collate_fn=collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cnm1m3HVCFsQ"
   },
   "source": [
    "Finally, we will let our model to train until convergence. Again, read the code carefully and understand every step. See the final plot and try to figure out what it is happening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4364045,
     "status": "ok",
     "timestamp": 1697813633248,
     "user": {
      "displayName": "Dani Fuertes",
      "userId": "03202983811684298813"
     },
     "user_tz": -120
    },
    "id": "-U_HsmJvAbBT",
    "outputId": "83e63deb-03c8-404b-931c-e70e3452328c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 53/53 [00:21<00:00,  2.48it/s]\n",
      "Val  : 100%|██████████| 8/8 [00:01<00:00,  5.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Train loss: 0.4379\n",
      " -> Valid loss: 0.1984 \n",
      "\n",
      "Epoch 2/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 53/53 [00:20<00:00,  2.54it/s]\n",
      "Val  : 100%|██████████| 8/8 [00:01<00:00,  5.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Train loss: 0.1703\n",
      " -> Valid loss: 0.1523 \n",
      "\n",
      "Epoch 3/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 53/53 [00:20<00:00,  2.55it/s]\n",
      "Val  : 100%|██████████| 8/8 [00:01<00:00,  5.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Train loss: 0.1195\n",
      " -> Valid loss: 0.1235 \n",
      "\n",
      "Epoch 4/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 53/53 [00:20<00:00,  2.54it/s]\n",
      "Val  : 100%|██████████| 8/8 [00:01<00:00,  5.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Train loss: 0.0980\n",
      " -> Valid loss: 0.1173 \n",
      "\n",
      "Epoch 5/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 53/53 [00:20<00:00,  2.54it/s]\n",
      "Val  : 100%|██████████| 8/8 [00:01<00:00,  5.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Train loss: 0.0808\n",
      " -> Valid loss: 0.1079 \n",
      "\n",
      "Epoch 6/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 53/53 [00:20<00:00,  2.54it/s]\n",
      "Val  : 100%|██████████| 8/8 [00:01<00:00,  5.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Train loss: 0.0669\n",
      " -> Valid loss: 0.1088 \n",
      "\n",
      "Epoch 7/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 53/53 [00:20<00:00,  2.54it/s]\n",
      "Val  : 100%|██████████| 8/8 [00:01<00:00,  5.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Train loss: 0.0623\n",
      " -> Valid loss: 0.1050 \n",
      "\n",
      "Epoch 8/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 53/53 [00:20<00:00,  2.54it/s]\n",
      "Val  : 100%|██████████| 8/8 [00:01<00:00,  5.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Train loss: 0.0579\n",
      " -> Valid loss: 0.1166 \n",
      "\n",
      "Epoch 9/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 53/53 [00:20<00:00,  2.54it/s]\n",
      "Val  : 100%|██████████| 8/8 [00:01<00:00,  5.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Train loss: 0.0535\n",
      " -> Valid loss: 0.1150 \n",
      "\n",
      "Epoch 10/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 53/53 [00:21<00:00,  2.52it/s]\n",
      "Val  : 100%|██████████| 8/8 [00:01<00:00,  5.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Train loss: 0.0465\n",
      " -> Valid loss: 0.1110 \n",
      "\n",
      "Epoch 11/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 53/53 [00:20<00:00,  2.54it/s]\n",
      "Val  : 100%|██████████| 8/8 [00:01<00:00,  5.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Train loss: 0.0458\n",
      " -> Valid loss: 0.0988 \n",
      "\n",
      "Epoch 12/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 53/53 [00:20<00:00,  2.54it/s]\n",
      "Val  : 100%|██████████| 8/8 [00:01<00:00,  5.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Train loss: 0.0420\n",
      " -> Valid loss: 0.1076 \n",
      "\n",
      "Epoch 13/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 53/53 [00:20<00:00,  2.54it/s]\n",
      "Val  : 100%|██████████| 8/8 [00:01<00:00,  5.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Train loss: 0.0385\n",
      " -> Valid loss: 0.1071 \n",
      "\n",
      "Epoch 14/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 53/53 [00:20<00:00,  2.53it/s]\n",
      "Val  : 100%|██████████| 8/8 [00:01<00:00,  5.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Train loss: 0.0371\n",
      " -> Valid loss: 0.1043 \n",
      "\n",
      "Epoch 15/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 53/53 [00:20<00:00,  2.53it/s]\n",
      "Val  : 100%|██████████| 8/8 [00:01<00:00,  5.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Train loss: 0.0363\n",
      " -> Valid loss: 0.1048 \n",
      "\n",
      "Epoch 16/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 53/53 [00:20<00:00,  2.54it/s]\n",
      "Val  : 100%|██████████| 8/8 [00:01<00:00,  5.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Train loss: 0.0360\n",
      " -> Valid loss: 0.1060 \n",
      "\n",
      "Epoch 17/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 53/53 [00:20<00:00,  2.54it/s]\n",
      "Val  : 100%|██████████| 8/8 [00:01<00:00,  5.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Train loss: 0.0317\n",
      " -> Valid loss: 0.1053 \n",
      "\n",
      "Epoch 18/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 53/53 [00:20<00:00,  2.54it/s]\n",
      "Val  : 100%|██████████| 8/8 [00:01<00:00,  5.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Train loss: 0.0295\n",
      " -> Valid loss: 0.1065 \n",
      "\n",
      "Epoch 19/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 53/53 [00:20<00:00,  2.54it/s]\n",
      "Val  : 100%|██████████| 8/8 [00:01<00:00,  5.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Train loss: 0.0276\n",
      " -> Valid loss: 0.1064 \n",
      "\n",
      "Epoch 20/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 53/53 [00:20<00:00,  2.54it/s]\n",
      "Val  : 100%|██████████| 8/8 [00:01<00:00,  5.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Train loss: 0.0275\n",
      " -> Valid loss: 0.1086 \n",
      "\n",
      "Epoch 21/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 53/53 [00:20<00:00,  2.54it/s]\n",
      "Val  : 100%|██████████| 8/8 [00:01<00:00,  5.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Train loss: 0.0302\n",
      " -> Valid loss: 0.1186 \n",
      "\n",
      "Epoch 22/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 53/53 [00:20<00:00,  2.54it/s]\n",
      "Val  : 100%|██████████| 8/8 [00:01<00:00,  5.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Train loss: 0.0307\n",
      " -> Valid loss: 0.1067 \n",
      "\n",
      "Epoch 23/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 53/53 [00:20<00:00,  2.54it/s]\n",
      "Val  : 100%|██████████| 8/8 [00:01<00:00,  5.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Train loss: 0.0252\n",
      " -> Valid loss: 0.1048 \n",
      "\n",
      "Epoch 24/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 53/53 [00:20<00:00,  2.54it/s]\n",
      "Val  : 100%|██████████| 8/8 [00:01<00:00,  5.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Train loss: 0.0285\n",
      " -> Valid loss: 0.1124 \n",
      "\n",
      "Epoch 25/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 53/53 [00:20<00:00,  2.55it/s]\n",
      "Val  : 100%|██████████| 8/8 [00:01<00:00,  5.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Train loss: 0.0296\n",
      " -> Valid loss: 0.1075 \n",
      "\n",
      "Epoch 26/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 53/53 [00:20<00:00,  2.54it/s]\n",
      "Val  : 100%|██████████| 8/8 [00:01<00:00,  5.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Train loss: 0.0267\n",
      " -> Valid loss: 0.1021 \n",
      "\n",
      "Epoch 27/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 53/53 [00:20<00:00,  2.54it/s]\n",
      "Val  : 100%|██████████| 8/8 [00:01<00:00,  5.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Train loss: 0.0246\n",
      " -> Valid loss: 0.1045 \n",
      "\n",
      "Epoch 28/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 53/53 [00:20<00:00,  2.55it/s]\n",
      "Val  : 100%|██████████| 8/8 [00:01<00:00,  5.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Train loss: 0.0231\n",
      " -> Valid loss: 0.1032 \n",
      "\n",
      "Epoch 29/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 53/53 [00:20<00:00,  2.54it/s]\n",
      "Val  : 100%|██████████| 8/8 [00:01<00:00,  5.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Train loss: 0.0213\n",
      " -> Valid loss: 0.0982 \n",
      "\n",
      "Epoch 30/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 53/53 [00:20<00:00,  2.54it/s]\n",
      "Val  : 100%|██████████| 8/8 [00:01<00:00,  5.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Train loss: 0.0223\n",
      " -> Valid loss: 0.1069 \n",
      "\n",
      "Epoch 31/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 53/53 [00:20<00:00,  2.54it/s]\n",
      "Val  : 100%|██████████| 8/8 [00:01<00:00,  5.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Train loss: 0.0211\n",
      " -> Valid loss: 0.1081 \n",
      "\n",
      "Epoch 32/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 53/53 [00:20<00:00,  2.54it/s]\n",
      "Val  : 100%|██████████| 8/8 [00:01<00:00,  5.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Train loss: 0.0219\n",
      " -> Valid loss: 0.0972 \n",
      "\n",
      "Epoch 33/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 53/53 [00:20<00:00,  2.54it/s]\n",
      "Val  : 100%|██████████| 8/8 [00:01<00:00,  5.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Train loss: 0.0255\n",
      " -> Valid loss: 0.1046 \n",
      "\n",
      "Epoch 34/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 53/53 [00:20<00:00,  2.55it/s]\n",
      "Val  : 100%|██████████| 8/8 [00:01<00:00,  5.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Train loss: 0.0265\n",
      " -> Valid loss: 0.1016 \n",
      "\n",
      "Epoch 35/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 53/53 [00:21<00:00,  2.51it/s]\n",
      "Val  : 100%|██████████| 8/8 [00:01<00:00,  4.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Train loss: 0.0238\n",
      " -> Valid loss: 0.1019 \n",
      "\n",
      "Epoch 36/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 53/53 [00:20<00:00,  2.54it/s]\n",
      "Val  : 100%|██████████| 8/8 [00:01<00:00,  5.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Train loss: 0.0208\n",
      " -> Valid loss: 0.1023 \n",
      "\n",
      "Epoch 37/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 53/53 [00:20<00:00,  2.55it/s]\n",
      "Val  : 100%|██████████| 8/8 [00:01<00:00,  5.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Train loss: 0.0232\n",
      " -> Valid loss: 0.1012 \n",
      "\n",
      "Epoch 38/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 53/53 [00:20<00:00,  2.54it/s]\n",
      "Val  : 100%|██████████| 8/8 [00:01<00:00,  5.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Train loss: 0.0221\n",
      " -> Valid loss: 0.1019 \n",
      "\n",
      "Epoch 39/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 53/53 [00:20<00:00,  2.55it/s]\n",
      "Val  : 100%|██████████| 8/8 [00:01<00:00,  5.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Train loss: 0.0236\n",
      " -> Valid loss: 0.1001 \n",
      "\n",
      "Epoch 40/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 53/53 [00:20<00:00,  2.54it/s]\n",
      "Val  : 100%|██████████| 8/8 [00:01<00:00,  4.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Train loss: 0.0214\n",
      " -> Valid loss: 0.0993 \n",
      "\n",
      "Epoch 41/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 53/53 [00:20<00:00,  2.53it/s]\n",
      "Val  : 100%|██████████| 8/8 [00:01<00:00,  5.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Train loss: 0.0204\n",
      " -> Valid loss: 0.0998 \n",
      "\n",
      "Epoch 42/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 53/53 [00:20<00:00,  2.54it/s]\n",
      "Val  : 100%|██████████| 8/8 [00:01<00:00,  5.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Train loss: 0.0183\n",
      " -> Valid loss: 0.1001 \n",
      "\n",
      "Epoch 43/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 53/53 [00:20<00:00,  2.55it/s]\n",
      "Val  : 100%|██████████| 8/8 [00:01<00:00,  5.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Train loss: 0.0174\n",
      " -> Valid loss: 0.0978 \n",
      "\n",
      "Epoch 44/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 53/53 [00:20<00:00,  2.54it/s]\n",
      "Val  : 100%|██████████| 8/8 [00:01<00:00,  5.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Train loss: 0.0176\n",
      " -> Valid loss: 0.1033 \n",
      "\n",
      "Epoch 45/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 53/53 [00:20<00:00,  2.54it/s]\n",
      "Val  : 100%|██████████| 8/8 [00:01<00:00,  5.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Train loss: 0.0175\n",
      " -> Valid loss: 0.1013 \n",
      "\n",
      "Epoch 46/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 53/53 [00:20<00:00,  2.55it/s]\n",
      "Val  : 100%|██████████| 8/8 [00:01<00:00,  5.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Train loss: 0.0170\n",
      " -> Valid loss: 0.1002 \n",
      "\n",
      "Epoch 47/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 53/53 [00:20<00:00,  2.54it/s]\n",
      "Val  : 100%|██████████| 8/8 [00:01<00:00,  5.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Train loss: 0.0168\n",
      " -> Valid loss: 0.1003 \n",
      "\n",
      "Epoch 48/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 53/53 [00:21<00:00,  2.52it/s]\n",
      "Val  : 100%|██████████| 8/8 [00:01<00:00,  5.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Train loss: 0.0177\n",
      " -> Valid loss: 0.1039 \n",
      "\n",
      "Epoch 49/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 53/53 [00:20<00:00,  2.55it/s]\n",
      "Val  : 100%|██████████| 8/8 [00:01<00:00,  5.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Train loss: 0.0182\n",
      " -> Valid loss: 0.0987 \n",
      "\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 53/53 [00:20<00:00,  2.54it/s]\n",
      "Val  : 100%|██████████| 8/8 [00:01<00:00,  5.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> Train loss: 0.0179\n",
      " -> Valid loss: 0.1005 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(history['current_epoch'], num_epochs):\n",
    "\n",
    "    # Set model in train mode (calculate gradients)\n",
    "    model.train()\n",
    "\n",
    "    # Iterate over each batch\n",
    "    loss_train_all = []\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    for images, targets in tqdm(train_loader, desc='Train'):\n",
    "\n",
    "        # Move data to device\n",
    "        images = torch.stack(images, dim=0).to(device)\n",
    "        targets = [{k: v.to(device) for k, v in targets[t].items()} for t in range(len(images))]\n",
    "\n",
    "        # Predict and get loss value\n",
    "        losses = model(images, targets)\n",
    "        loss_train = sum(loss for loss in losses.values()).mean()\n",
    "        loss_train_all.append(loss_train)\n",
    "\n",
    "        # Reset gradients\n",
    "        ...\n",
    "\n",
    "        # Apply backpropagation\n",
    "        ...\n",
    "\n",
    "        # Clip gradients (to avoid gradient exploding)\n",
    "        clip_grad_norms(optimizer.param_groups, max_norm=max_grad_norm)\n",
    "\n",
    "        # Update model's weights\n",
    "        ...\n",
    "    \n",
    "    # Get average loss for the epoch\n",
    "    loss_train = torch.tensor(loss_train_all, device=device).mean()\n",
    "\n",
    "    # Update scheduler\n",
    "    lr_scheduler.step(loss_train)\n",
    "\n",
    "    # Validation (gradients are not necessary)\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # Validation\n",
    "        loss_valid_all = []\n",
    "        for images, targets in tqdm(valid_loader, desc='Val  '):\n",
    "\n",
    "            # Move data to device\n",
    "            images = torch.stack(images, dim=0).to(device)\n",
    "            targets = [{k: v.to(device) for k, v in targets[t].items()} for t in range(len(images))]\n",
    "\n",
    "            # Predict and get loss value\n",
    "            losses = model(images, targets)\n",
    "            loss_valid = sum(loss for loss in losses.values()).mean()\n",
    "            loss_valid_all.append(loss_valid)\n",
    "        loss_valid = torch.tensor(loss_valid_all, device=device).mean()\n",
    "\n",
    "    # Save training history\n",
    "    history['current_epoch'] = epoch + 1\n",
    "    history['train_loss'].append(np.mean([loss.detach().cpu().numpy() for loss in loss_train_all]))\n",
    "    history['valid_loss'].append(np.mean([loss.detach().cpu().numpy() for loss in loss_valid_all]))\n",
    "    print(f\" -> Train loss: {history['train_loss'][-1]:.4f}\")\n",
    "    print(f\" -> Valid loss: {history['valid_loss'][-1]:.4f} \\n\")\n",
    "\n",
    "    # Save best model\n",
    "    if loss_train < history['best_value']:\n",
    "        history['best_value'] = loss_train\n",
    "        torch.save(\n",
    "            {\n",
    "                'model': model.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'rng_state': torch.get_rng_state(),\n",
    "                'cuda_rng_state': torch.cuda.get_rng_state_all(),\n",
    "                'history': history,\n",
    "            },\n",
    "            os.path.join(model_path)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 452
    },
    "executionInfo": {
     "elapsed": 879,
     "status": "ok",
     "timestamp": 1697813634112,
     "user": {
      "displayName": "Dani Fuertes",
      "userId": "03202983811684298813"
     },
     "user_tz": -120
    },
    "id": "z7Sg81RYCkZ-",
    "outputId": "7d5f12c1-124d-4af9-f915-38e9c8dc2512"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot training history\n",
    "epochs = range(1, max(len(history['train_loss']), len(history['valid_loss'])) + 1)\n",
    "plt.plot(epochs, history['train_loss'], label='train_loss')\n",
    "plt.plot(epochs, history['valid_loss'], label='valid_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.savefig(os.path.join(save_dir, 'history.png'))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
